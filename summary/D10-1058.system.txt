We built a fertility hidden Markov model by adding fertility to the hidden Markov model. We use Gibbs sampling for parameter estimation, which is more principled than the neighborhood method used in IBM Model 4. IBM models and the hidden Markov model (HMM) for word alignment are the most influential statistical word alignment models (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003). Recent experiments on large datasets have shown that the performance of the hidden Markov model is very close to IBM Model 4. Our work is different from others in essential ways. We use Gibbs sampling instead of a heuristic-based neighborhood method for parameter 596 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 596–605, MIT, Massachusetts, USA, 911 October 2010. Qc 2010 Association for Computational Linguistics estimation. Parameter estimation for word alignment models that model fertility is more difficult than for models without fertility. We use the Markov Chain Monte Carlo (MCMC) method for training and decoding, φi = j=1 δ(aj , i) which has nice probabilistic guarantees. (2008) applied the Markov Chain Monte Carlo method to word alignment for machine translation; they do not model word fertility. The Markov Chain Monte Carlo method used in our model is more principled than the heuristic-based neighborhood method in IBM Model 4.